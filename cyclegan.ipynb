{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环生成式对抗网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN（循环对抗生成网络，Cycle Generative Adversarial Networks）是GAN的扩展。\n",
    "\n",
    "GAN网络对于图片的风格转换是很好的，但是获取成对的训练数据可能是困难和昂贵的。例如，对于语义分割等任务，只有几个数据集存在，而且它们相对较小。获得像艺术风格化这样的图形任务的输入-输出对可能会更加困难，因为所需的输出是非常复杂的，通常需要艺术创作。所以文章里面实现了捕获一个图像集合的特殊特征，并找出这些特征如何转换成另一个图像集合，所有这些都在没有任何成对训练例子的情况下。\n",
    "\n",
    "其生成器模型参考了Perceptual losses for real-time style transfer and super-resolution这篇论文的网络架构，判别器采用了70×70 PatchGANs[22, 30, 29]结构，相对于GAN其生成器里面利用了多个上卷积下卷积和残差块级联的结合，其基本原理如下所示，相较于传统的GAN网络采用了正向反向GAN网络的循环一致性将其加入了损失函数，同时训练两个方向的GAN网络。\n",
    "\n",
    "![cyclemodel](./images/model.jpg)\n",
    "\n",
    "我们在域$X$中得到一组图像，在域$Y$中得到一组不同的图像。我们可以训练一个映射$G：X→Y$，这样输出的$yˆ=G(x)$，$x∈X$，与图像$y∈Y$无法区分。理论上，这个目标可以在$yˆ$上诱导一个输出分布，与经验分布$p_{data(y)}$相匹配(一般来说，这需要$G$是随机的)。因此，最优$G$将域$X$转化为与$Y$相同分布的域$Yˆ$。然而，这样的转换并不能保证单个的输入$x$和输出$y$以一种有意义的方式配对，这会导致有无限多个映射$G$将在$yˆ$上诱导相同的分布。此外，在实践中，我们发现很难孤立地优化对抗性目标。\n",
    "\n",
    "例如，把一个句子从英语翻译成法语，然后把它从法语翻译回英语，我们应该回到原来的句子。从数学上讲，如果我们有一个翻译$G：X→Y$和另一个翻译$F：Y→X$，那么$G$和$F$应该是相反的，并且两个映射都应该是双射。我们通过将映射G和F同时训练，并增加一个循环一致性损失来激励$F(G(x))\\approx x$和$G(F(y))\\approx y$。将这种损失与域$X$和$Y$上的对抗性损失相结合，可以得到我们的未配对图像到图像转换的完整目标。\n",
    "\n",
    "它由Jun-Yan Zhu等人在论文[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593.pdf)中进行描述。判别器由卷积层、BatchNorm层和LeakyRelu激活层以及组成。输入是生成器的的返回参数，输出是一个对应维度的tensor。生成器则是由转置卷积层、BatchNorm层和ReLU激活层以及重要的残差块级联组成。输入是3x256x256（3x128x128），输出是fake_A, fake_B, rec_A, rec_B, identity_A, identity_B。\n",
    "\n",
    "本教程将使用论文作者提供的taesung_park/CycleGAN数据集来训练一个生成式对抗网络，接着使用该网络进行对应的风格转换。\n",
    "\n",
    "![cycleresult](./images/result.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的教程中，我们将通过示例代码说明如何设置网络、优化器、如何计算损失函数以及如何初始化模型权重。在本教程中，我们使用taesung_park/CycleGAN数据集。首先我们将数据集下载到指定目录下并解压。示例代码如下（同样可以使用提供的sh文件进行下载），horse2zebra数据集进行展示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "\n",
    "开始实验之前，请确保本地已经安装了Mindspore环境并安装了MindSpore Vision套件。\n",
    "首先通过args获得默认配置的参数，选择phase参数默认为训练模式，并选择执行模式，在每个模块中重要的参数将会在后面的模块使用中进行介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "from mindspore import context\n",
    "\n",
    "from src.config.cyclegan_config import parse_args\n",
    "\n",
    "\n",
    "# 这里通过args获取必要参数并且进行执行模式的设置。\n",
    "args = parse_args(\"train\")\n",
    "\n",
    "# 选择执行模式为图模式；指定训练使用的平台为\"GPU\"，如需使用昇腾硬件可将其替换为\"Ascend\"。\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "\n",
    "args.rank = 0\n",
    "args.device_num = 1\n",
    "\n",
    "# 这里在\"GPU\"模式下，启用图算融合去优化网络。\n",
    "if args.platform == \"GPU\":\n",
    "    context.set_context(enable_graph_kernel=True)\n",
    "\n",
    "# 如果是在\"Ascend\"训练平台下，则将填充模式设置为\"CONSTANT\"。\n",
    "if args.platform == \"Ascend\":\n",
    "    args.pad_mode = \"CONSTANT\"\n",
    "\n",
    "# 这里如果batch_size为1，我们设置为\"instance\"，如果batch_size为其他则设置为\"batch\"模式，本文章默认为1的效果更加佳。\n",
    "if args.batch_size == 1:\n",
    "    args.norm_mode = \"instance\"\n",
    "\n",
    "if args.dataroot is None:\n",
    "    name, _ = os.path.splitext(args.data)\n",
    "    data_path = \"./data/\"\n",
    "    args.dataroot = os.path.join(data_path, name)\n",
    "\n",
    "if args.max_dataset_size is None:\n",
    "    args.max_dataset_size = float(\"inf\")\n",
    "\n",
    "args.max_epoch = 1  #此处将训练轮次设置为1。\n",
    "args.n_epochs = min(args.max_epoch, args.n_epochs)\n",
    "args.n_epochs_decay = args.max_epoch - args.n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在进行数据集的下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116868096B [03:07, 623186.81B/s]                                                \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mindvision import dataset\n",
    "\n",
    "\n",
    "dl_path = \"./data/\"\n",
    "path_1 = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\"\n",
    "dl_url = os.path.join(path_1, args.data)\n",
    "dl = dataset.DownLoad()\n",
    "dl.download_and_extract_archive(url=dl_url, download_path=dl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以在args.py文件中将data参数的horse2zebra.zip替换为：\n",
    "\n",
    "1.apple2orange.zip ,苹果和橘子间的转换。\n",
    "\n",
    "2.summer2winter_yosemite.zip, 夏天和冬天的转换。\n",
    "\n",
    "3.monet2photo.zip, monet风格和照片的转换。\n",
    "\n",
    "4.cezanne2photo.zip, cezanne风格和照片的转换。\n",
    "\n",
    "5.ukiyoe2photo.zip, ukiyoe风格和照片的转换。\n",
    "\n",
    "6.vangogh2photo.zip, vangogh风格和照片的转换。\n",
    "\n",
    "进行其他风格的迁移训练，但需要保证其文件夹包含trainA，trainB，testA，testB，其中test文件是用于进行推理的，如果解压之后没有可以自己自定义图片进行对应的风格转换。数据集的下载地址为：https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\n",
    "\n",
    "下载后的数据集目录结构如下：\n",
    "\n",
    "```text\n",
    "./data/horse2zebra\n",
    "├── trainA\n",
    "├── trainB\n",
    "├── testA\n",
    "└── testB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对图片数据集读取，进行打散，以及数据增强操作，最终产生数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:***********Setting rank to 0 since it is not passed in ******************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import mindspore.dataset as de\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "\n",
    "from src.process_datasets.dataset import UnalignedDataset\n",
    "from src.process_datasets.distributed_sampler import DistributedSampler\n",
    "\n",
    "\n",
    "# 以下操作可以通过引用cyclegan_dataset中的create_dataset进行一步完成。\n",
    "# 此处通过平台核数计算并行工作数以及定义数据增强操作的参数。\n",
    "cores = multiprocessing.cpu_count()\n",
    "num_parallel_workers = min(8, int(cores / args.device_num))\n",
    "mean = [0.5 * 255] * 3\n",
    "std = [0.5 * 255] * 3\n",
    "\n",
    "# 这里进行了数据集的加载。\n",
    "dataset = UnalignedDataset(args.dataroot, args.phase, max_dataset_size=args.max_dataset_size, use_random=args.use_random)\n",
    "\n",
    "# 这里将数据集进行打乱。\n",
    "distributed_sampler = DistributedSampler(len(dataset), args.device_num, args.rank, shuffle=args.use_random)\n",
    "\n",
    "# 这里将数据及进行生成。\n",
    "ds = de.GeneratorDataset(dataset, column_names=[\"image_A\", \"image_B\"],\n",
    "                         sampler=distributed_sampler, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "# 这里定义了数据增强，数据裁剪操作，在传参默认使用了use_random，同时将A域B域的图片进行相同的处理。\n",
    "trans = [\n",
    "    C.RandomResizedCrop(args.image_size, scale=(0.5, 1.0), ratio=(0.75, 1.333)),\n",
    "    C.RandomHorizontalFlip(prob=0.5),\n",
    "    C.Normalize(mean=mean, std=std),\n",
    "    C.HWC2CHW()\n",
    "]\n",
    "ds = ds.map(operations=trans, input_columns=[\"image_A\"], num_parallel_workers=num_parallel_workers)\n",
    "ds = ds.map(operations=trans, input_columns=[\"image_B\"], num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "# 此处的batch_size默认设为1，采用实例模式和eval模式保持一致。\n",
    "ds = ds.batch(args.batch_size, drop_remainder=True) \n",
    "args.dataset_size = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过create_dict_iterator函数将数据转换成字典迭代器，然后使用matplotlib模块可视化部分训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_iter = next(ds.create_dict_iterator(output_numpy=True))\n",
    "\n",
    "# 可视化部分训练数据\n",
    "plt.figure(figsize=(10, 3), dpi=140)\n",
    "for i, image in enumerate(data_iter['image'][:30], 1):\n",
    "    plt.subplot(3, 10, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建网络\n",
    "\n",
    "当处理完数据后，就可以来进行网络的搭建了。\n",
    "\n",
    "#### 生成器\n",
    "\n",
    "生成器本案例采用resnet结构的生成器，由于图片数据集在数据处理过程中默认为处理成256分辨率大小，所以n_layers也默认设置为9，表示了其网络有9个残差块相连，此处来源于文章中说过，对于128的图片采用6个残差块相连，256分辨率以上的采用9个参差块相连。\n",
    "\n",
    "cyclegan论文论文中生成器结构如下所示：\n",
    "\n",
    "![cyclegangenerator](./images/generator.jpg)\n",
    "\n",
    "其具体的参数配置请参照具体代码\n",
    "\n",
    "我们通过输入部分中设置的`n_layers`和`ngf`来影响代码中的生成器结构。`ngf`是输出通道数也是滤波器数，`n_layers`是级联的残差块数。\n",
    "\n",
    "以下是生成器的代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "from src.models.convrelu import ConvNormReLU, ConvTransposeNormReLU\n",
    "from src.models.block import ResidualBlock\n",
    "\n",
    "\n",
    "# 这里定义了ResNet生成器的网络结构，其中ConvNormReLU, ConvTransposeNormReLU层是定义的卷积以及BatchNorm加上ReLU的一个集合层。\n",
    "# 此处通过n_layers参数来实现选择几个残差块作为相连，来处理不同尺寸的图片。\n",
    "class ResNetGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResNet Generator of GAN.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Input channel. Default: 3.\n",
    "        ngf (int): Output channel. Default: 64.\n",
    "        n_layers (int): The number of ConvNormReLU blocks. Default: 9.\n",
    "        alpha (float): LeakyRelu slope. Default: 0.2.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\". Default: \"batch\".\n",
    "        dropout (bool): Use dropout or not. Default: False.\n",
    "        pad_mode (str): Specifies padding mode. The optional values are \"CONSTANT\", \"REFLECT\", \"SYMMETRIC\".\n",
    "            Default: \"CONSTANT\".\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes=3, ngf=64, n_layers=9, alpha=0.2, norm_mode='batch', dropout=False,\n",
    "                 pad_mode=\"CONSTANT\"):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        self.conv_in = ConvNormReLU(in_planes, ngf, 7, 1, alpha, norm_mode, pad_mode=pad_mode)\n",
    "        self.down_1 = ConvNormReLU(ngf, ngf * 2, 3, 2, alpha, norm_mode)\n",
    "        self.down_2 = ConvNormReLU(ngf * 2, ngf * 4, 3, 2, alpha, norm_mode)\n",
    "        layers = [ResidualBlock(ngf * 4, norm_mode, dropout=dropout, pad_mode=pad_mode)] * n_layers\n",
    "        self.residuals = nn.SequentialCell(layers)\n",
    "        self.up_2 = ConvTransposeNormReLU(ngf * 4, ngf * 2, 3, 2, alpha, norm_mode)\n",
    "        self.up_1 = ConvTransposeNormReLU(ngf * 2, ngf, 3, 2, alpha, norm_mode)\n",
    "        if pad_mode == \"CONSTANT\":\n",
    "            self.conv_out = nn.Conv2d(ngf, 3, kernel_size=7, stride=1, pad_mode='pad', padding=3)\n",
    "        else:\n",
    "            pad = nn.Pad(paddings=((0, 0), (0, 0), (3, 3), (3, 3)), mode=pad_mode)\n",
    "            conv = nn.Conv2d(ngf, 3, kernel_size=7, stride=1, pad_mode='pad')\n",
    "            self.conv_out = nn.SequentialCell([pad, conv])\n",
    "        self.activate = ops.Tanh()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.down_1(x)\n",
    "        x = self.down_2(x)\n",
    "        x = self.residuals(x)\n",
    "        x = self.up_2(x)\n",
    "        x = self.up_1(x)\n",
    "        output = self.conv_out(x)\n",
    "        return self.activate(output)\n",
    "\n",
    "# 这里定义了实例化生成器以及初始化权重的函数。\n",
    "def get_generator(args):\n",
    "    \"\"\"\n",
    "    This will implement the CycleGAN model, for learning image-to-image translation without paired data.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): in_planes. Default: 3.\n",
    "        ngf (int): generator model filter numbers. Default: 64.\n",
    "        gl_num (int): generator model residual block numbers. Default: 9.\n",
    "        alpha (float): leakyrelu slope. Default: 0.02.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\". Default: \"batch\".\n",
    "        dropout (bool): Use dropout or not. Default: False.\n",
    "        pad_mode(str): the type of Pad. The optional values are 'CONSTANT', 'REFLECT', 'SYMMETRIC'. Default: \"CONSTANT\".\n",
    "        init_type(str): network initialization. The optional values are 'normal', 'xavier'. Default: 'normal'.\n",
    "        init_gain(float): scaling factor for normal, xavier and orthogonal. Default: 0.02.\n",
    "\n",
    "    Returns:\n",
    "        nn.Cell.\n",
    "    \"\"\"\n",
    "\n",
    "    net = ResNetGenerator(in_planes=args.in_planes, ngf=args.ngf, n_layers=args.gl_num,\n",
    "                          alpha=args.slope, norm_mode=args.norm_mode, dropout=args.need_dropout,\n",
    "                          pad_mode=args.pad_mode)\n",
    "    init_weights(net, args.init_type, args.init_gain)\n",
    "\n",
    "    return net\n",
    "\n",
    "# 这里定义了CycleGAN生成器，用来返回用于计算损失函数的各个参数。\n",
    "class Generator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Generator of CycleGAN, return fake_a, fake_b, rec_a, rec_b, identity_a and identity_b.\n",
    "\n",
    "    Args:\n",
    "        g_a (Cell): The generator network of domain a to domain b.\n",
    "        g_b (Cell): The generator network of domain b to domain a.\n",
    "        use_identity (bool): Use identity loss or not. Default: True.\n",
    "\n",
    "    Returns:\n",
    "        Tensors, fake_a, fake_b, rec_a, rec_b, identity_a and identity_b.\n",
    "\n",
    "    Examples:\n",
    "        >>> Generator(g_a, g_b)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g_a, g_b, use_identity=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.g_a = g_a\n",
    "        self.g_b = g_b\n",
    "        self.ones = ops.OnesLike()\n",
    "        self.use_identity = use_identity\n",
    "\n",
    "    def construct(self, img_a, img_b):\n",
    "        \"\"\"If use_identity, identity loss will be used.\"\"\"\n",
    "        fake_a = self.g_b(img_b)\n",
    "        fake_b = self.g_a(img_a)\n",
    "        rec_a = self.g_b(fake_b)\n",
    "        rec_b = self.g_a(fake_a)\n",
    "        if self.use_identity:\n",
    "            identity_a = self.g_b(img_a)\n",
    "            identity_b = self.g_a(img_b)\n",
    "        else:\n",
    "            identity_a = self.ones(img_a)\n",
    "            identity_b = self.ones(img_b)\n",
    "        return fake_a, fake_b, rec_a, rec_b, identity_a, identity_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 判别器\n",
    "\n",
    "如前所述，判别器`D`是一个二分类网络模型，输出判定该图像为真实图的概率。通过一系列的`Conv2d`、`BatchNorm2d`和`LeakyReLU`层对其进行处理，最后通过`Sigmoid`激活函数得到最终概率。\n",
    "\n",
    "cyclegan论文提到，判别器使用Patch大小为70 * 70的PatchGAN。\n",
    "\n",
    "判别器的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "from src.models.convrelu import ConvNormReLU\n",
    "from src.utils.init_weight import init_weights\n",
    "\n",
    "\n",
    "# 这里定义了PatchGAN判别器。\n",
    "class Discriminator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Discriminator of GAN.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Input channel. Default: 3.\n",
    "        ndf (int): discriminator model filter numbers. Default: 64.\n",
    "        n_layers (int): The number of ConvNormReLU blocks. Default: 3.\n",
    "        alpha (float): LeakyRelu slope. Default: 0.2.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\". Default: \"batch\".\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> Discriminator(3, 64, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes=3, ndf=64, n_layers=3, alpha=0.2, norm_mode='batch'):\n",
    "        super(Discriminator, self).__init__()\n",
    "        kernel_size = 4\n",
    "        layers = [\n",
    "            nn.Conv2d(in_planes, ndf, kernel_size, 2, pad_mode='pad', padding=1),\n",
    "            nn.LeakyReLU(alpha)\n",
    "        ]\n",
    "        nf_mult = ndf\n",
    "        for i in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** i, 8) * ndf\n",
    "            layers.append(ConvNormReLU(nf_mult_prev, nf_mult, kernel_size, 2, alpha, norm_mode, padding=1))\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8) * ndf\n",
    "        layers.append(ConvNormReLU(nf_mult_prev, nf_mult, kernel_size, 1, alpha, norm_mode, padding=1))\n",
    "        layers.append(nn.Conv2d(nf_mult, 1, kernel_size, 1, pad_mode='pad', padding=1))\n",
    "        self.features = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        output = self.features(x)\n",
    "        return output\n",
    "\n",
    "# 这里定义了初始化权重以及实例化函数。\n",
    "def get_discriminator(args):\n",
    "    \"\"\"\n",
    "    This will return discriminator by args\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): in_planes. Default: 3.\n",
    "        ndf (int): discriminator model filter numbers. Default: 64.\n",
    "        gl_num (int): generator model residual block numbers. Default: 9.\n",
    "        alpha (float): leakyrelu slope. Default: 0.02.\n",
    "        norm_mode (str): Specifies norm method. The optional values are \"batch\", \"instance\". Default: \"batch\".\n",
    "        init_type(str): network initialization. The optional values are 'normal', 'xavier'. Default: 'normal'.\n",
    "        init_gain(float): scaling factor for normal, xavier and orthogonal. Default: 0.02.\n",
    "\n",
    "    Returns:\n",
    "        nn.Cell.\n",
    "    \"\"\"\n",
    "\n",
    "    net = Discriminator(in_planes=args.in_planes, ndf=args.ndf, n_layers=args.dl_num,\n",
    "                        alpha=args.slope, norm_mode=args.norm_mode)\n",
    "    init_weights(net, args.init_type, args.init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数\n",
    "\n",
    "对于映射函数G：X→Y及其鉴别器DY，我们将目标损失函数定义为:\n",
    "\n",
    "$$\\qquad\\qquad\\qquad\\qquad\\qquad L_{GAN}(G,D_Y,X,Y)=E_{y-p_{data}(y)}[logD_Y(y)]+E_{x-p_{data}(x)}[log(1-D_Y(G(x)))]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(1)$$\n",
    "\n",
    "其中G试图生成与域Y的图像相似的图像$G(x)$，而$D_Y$的目的是区分生成样本$G(x)$和真实样本$y$，生成器的目标是最小化这个损失函数以此来对抗判别器。\n",
    "为了进一步减少可能的映射函数的空间，文章中认为学习到的映射函数应该是周期一致的，例如对于域$X$中的每一个图像$x$，图像平移一个周期映射会原始图像，即$x→G(x)→F(G(x))\\approx x$，从而文章采用了一个循环一致性损失来激励这种行为。\n",
    "\n",
    "$$\\qquad\\qquad\\qquad\\qquad\\qquad L_{cyc}(G,F)=E_{x-p_{data}(x)}[\\Vert F(G(x))-x\\Vert_{1}]+E_{y-p_{data}(y)}[\\Vert G(F(y))-y\\Vert_{1}]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(2)$$\n",
    "\n",
    "文章中提出来的总的损失函数如下，其中包含了正向以及反向GAN网络损失函数，另外加上了一个循环一致性。\n",
    "\n",
    "$$\\qquad\\qquad\\qquad\\qquad\\qquad L(G,F,D_X,D_Y)=L_{GAN}(G,D_Y,X,Y)+L_{GAN}(F,D_X,Y,X)+\\lambda L_{cyc}(G,F)\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\quad(3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import Tensor\n",
    "from mindvision.check_param import Validator\n",
    "\n",
    "\n",
    "# 连接BCEWithLogits损失函数。\n",
    "class BCEWithLogits(nn.Cell):\n",
    "    \"\"\"\n",
    "    BCEWithLogits creates a criterion to measure the Binary Cross Entropy between the true labels and\n",
    "    predicted labels with sigmoid logits.\n",
    "\n",
    "    Args:\n",
    "        reduction (str): Specifies the reduction to be applied to the output. The optional values are 'none', 'mean', 'sum'. Default: 'none'.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor or Scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(BCEWithLogits, self).__init__()\n",
    "        if reduction is None:\n",
    "            reduction = 'none'\n",
    "        if not reduction:\n",
    "            Validator.check_string(reduction, ['none', 'mean', 'sum'], \"reduction\")\n",
    "\n",
    "        self.loss = ops.SigmoidCrossEntropyWithLogits()\n",
    "        self.reduce = False\n",
    "        if reduction == 'sum':\n",
    "            self.reduce_mode = ops.ReduceSum()\n",
    "            self.reduce = True\n",
    "        elif reduction == 'mean':\n",
    "            self.reduce_mode = ops.ReduceMean()\n",
    "            self.reduce = True\n",
    "    def construct(self, predict, target):\n",
    "        loss = self.loss(predict, target)\n",
    "        if self.reduce:\n",
    "            loss = self.reduce_mode(loss)\n",
    "        return loss\n",
    "\n",
    "# GAN网络损失函数，这里最后一层不使用sigmod函数。\n",
    "class GANLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "    The GANLoss class abstracts away the need to create the target label tensor that has the same size as the input.\n",
    "\n",
    "    Args:\n",
    "        mode (str): The type of GAN objective. It currently supports 'vanilla', 'lsgan'. Default: 'lsgan'.\n",
    "        reduction (str): Specifies the reduction to be applied to the output.\n",
    "            Its value must be one of 'none', 'mean', 'sum'. Default: 'mean'.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor or Scalar, if `reduction` is 'none', then output is a tensor and has the same shape as `inputs`.\n",
    "        Otherwise, the output is a scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"lsgan\", reduction='mean'):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.loss = None\n",
    "        self.ones = ops.OnesLike()\n",
    "        if mode == \"lsgan\":\n",
    "            self.loss = nn.MSELoss(reduction)\n",
    "        elif mode == \"vanilla\":\n",
    "            self.loss = BCEWithLogits(reduction)\n",
    "        else:\n",
    "            raise NotImplementedError(f'GANLoss {mode} not recognized, we support lsgan and vanilla.')\n",
    "\n",
    "    def construct(self, predict, target):\n",
    "        target = ops.cast(target, ops.dtype(predict))\n",
    "        target = self.ones(predict) * target\n",
    "        loss = self.loss(predict, target)\n",
    "        return loss\n",
    "\n",
    "# 这里定义了生成器损失函数。\n",
    "class GeneratorLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "    Cycle GAN generator loss.\n",
    "\n",
    "    Args:\n",
    "        args (class): Option class.\n",
    "        generator (Cell): Generator of CycleGAN.\n",
    "        d_a (Cell): The discriminator network of domain a to domain b.\n",
    "        d_b (Cell): The discriminator network of domain b to domain a.\n",
    "\n",
    "    Outputs:\n",
    "        Tuple Tensor, the losses of generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, generator, d_a, d_b):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        self.lambda_a = args.lambda_a\n",
    "        self.lambda_b = args.lambda_b\n",
    "        self.lambda_idt = args.lambda_idt\n",
    "        self.use_identity = args.lambda_idt > 0\n",
    "        self.dis_loss = GANLoss(args.gan_mode)\n",
    "        self.rec_loss = nn.L1Loss(\"mean\")\n",
    "        self.generator = generator\n",
    "        self.d_a = d_a\n",
    "        self.d_b = d_b\n",
    "        self.true = Tensor(True, mstype.bool_)\n",
    "\n",
    "    def construct(self, img_a, img_b):\n",
    "        fake_a, fake_b, rec_a, rec_b, identity_a, identity_b = self.generator(img_a, img_b)\n",
    "        loss_g_a = self.dis_loss(self.d_b(fake_b), self.true)\n",
    "        loss_g_b = self.dis_loss(self.d_a(fake_a), self.true)\n",
    "        loss_c_a = self.rec_loss(rec_a, img_a) * self.lambda_a\n",
    "        loss_c_b = self.rec_loss(rec_b, img_b) * self.lambda_b\n",
    "        if self.use_identity:\n",
    "            loss_idt_a = self.rec_loss(identity_a, img_a) * self.lambda_a * self.lambda_idt\n",
    "            loss_idt_b = self.rec_loss(identity_b, img_b) * self.lambda_b * self.lambda_idt\n",
    "        else:\n",
    "            loss_idt_a = 0\n",
    "            loss_idt_b = 0\n",
    "        loss_g = loss_g_a + loss_g_b + loss_c_a + loss_c_b + loss_idt_a + loss_idt_b\n",
    "        return (fake_a, fake_b, loss_g, loss_g_a, loss_g_b, loss_c_a, loss_c_b, loss_idt_a, loss_idt_b)\n",
    "\n",
    "# 这里定义了判别器的损失函数。\n",
    "class DiscriminatorLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "    Cycle GAN discriminator loss.\n",
    "\n",
    "    Args:\n",
    "        args (class): option class.\n",
    "        d_a (Cell): The discriminator network of domain a to domain b.\n",
    "        d_b (Cell): The discriminator network of domain b to domain a.\n",
    "\n",
    "    Outputs:\n",
    "        Tuple Tensor, the loss of discriminator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, d_a, d_b):\n",
    "        super(DiscriminatorLoss, self).__init__()\n",
    "        self.d_a = d_a\n",
    "        self.d_b = d_b\n",
    "        self.false = Tensor(False, mstype.bool_)\n",
    "        self.true = Tensor(True, mstype.bool_)\n",
    "        self.dis_loss = GANLoss(args.gan_mode)\n",
    "        self.rec_loss = nn.L1Loss(\"mean\")\n",
    "\n",
    "    def construct(self, img_a, img_b, fake_a, fake_b):\n",
    "        d_fake_a = self.d_a(fake_a)\n",
    "        d_img_a = self.d_a(img_a)\n",
    "        d_fake_b = self.d_b(fake_b)\n",
    "        d_img_b = self.d_b(img_b)\n",
    "        loss_d_a = self.dis_loss(d_fake_a, self.false) + self.dis_loss(d_img_a, self.true)\n",
    "        loss_d_b = self.dis_loss(d_fake_b, self.false) + self.dis_loss(d_img_b, self.true)\n",
    "        loss_d = (loss_d_a + loss_d_b) * 0.5\n",
    "        return loss_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装网络和损失函数\n",
    "\n",
    "MindSpore将损失函数、优化器等操作都封装到了Cell中，因为GAN结构上的特殊性，其损失是判别器和生成器的多输出形式，这就导致它和一般的分类网络不同。所以我们需要自定义`WithLossCell`类，将网络和Loss连接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里自定义了封装损失和网络模型的类。\n",
    "class WithLossCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Wrap the network with loss function to return generator loss.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The target network to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, img_a, img_b):\n",
    "        lg = (self.network(img_a, img_b))[2]\n",
    "        return lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "训练分为两个主要部分：训练判别器和训练生成器，在前文的损失函数（1）中，文章采用了最小二乘损失代替负对数似然目标。\n",
    "\n",
    "- 训练判别器\n",
    "\n",
    "   训练判别器的目的是最大程度地提高判别图像真伪的概率。按照文章的方法需要训练判别器来最小化$E_{y-p_{data}(y)}[(D(y)-1)^2]$\n",
    "\n",
    "- 训练生成器\n",
    "\n",
    "   如cyclegan论文所述，我们希望通过最小化$E_{x-p_{data}(x)}[(D(G(x)-1)^2]$来训练生成器，以产生更好的虚假图像。\n",
    "\n",
    "下面定义了生成器和判别器的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.parallel._auto_parallel_context import auto_parallel_context\n",
    "from mindspore.communication.management import get_group_size\n",
    "\n",
    "\n",
    "# 这里定义了生成器的训练过程。\n",
    "class TrainOneStepG(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of Cycle GAN generator network training.\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "    Args:\n",
    "        g (Cell): Generator with loss Cell. Note that loss function should have been added.\n",
    "        generator (Cell): Generator of CycleGAN.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g, generator, optimizer, sens=1.0):\n",
    "        super(TrainOneStepG, self).__init__(auto_prefix=False)\n",
    "        self.optimizer = optimizer\n",
    "        self.g = g\n",
    "        self.g.set_grad()\n",
    "        self.g.set_train()\n",
    "        self.g.d_a.set_grad(False)\n",
    "        self.g.d_a.set_train(False)\n",
    "        self.g.d_b.set_grad(False)\n",
    "        self.g.d_b.set_train(False)\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.weights = ms.ParameterTuple(generator.trainable_params())\n",
    "        self.net = WithLossCell(g)\n",
    "        self.reducer_flag = False\n",
    "        self.grad_reducer = None\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            if auto_parallel_context().get_device_num_is_set():\n",
    "                degree = context.get_auto_parallel_context(\"device_num\")\n",
    "            else:\n",
    "                degree = get_group_size()\n",
    "            self.grad_reducer = nn.DistributedGradReducer(optimizer.parameters, mean, degree)\n",
    "\n",
    "    def construct(self, img_a, img_b):\n",
    "        weights = self.weights\n",
    "        fake_a, fake_b, lg, lga, lgb, lca, lcb, lia, lib = self.g(img_a, img_b)\n",
    "        sens = ops.Fill()(ops.DType()(lg), ops.Shape()(lg), self.sens)\n",
    "        grads_g = self.grad(self.net, weights)(img_a, img_b, sens)\n",
    "        if self.reducer_flag:\n",
    "            # apply grad reducer on grads\n",
    "            grads_g = self.grad_reducer(grads_g)\n",
    "\n",
    "        return fake_a, fake_b, ops.depend(lg, self.optimizer(grads_g)), lga, lgb, lca, lcb, lia, lib\n",
    "\n",
    "# 这里定义了判别器的训练过程。\n",
    "class TrainOneStepD(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of Cycle GAN discriminator network training.\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "    Args:\n",
    "        G (Cell): Generator with loss Cell. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, optimizer, sens=1.0):\n",
    "        super(TrainOneStepD, self).__init__(auto_prefix=False)\n",
    "        self.optimizer = optimizer\n",
    "        self.d = d\n",
    "        self.d.set_grad()\n",
    "        self.d.set_train()\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.weights = ms.ParameterTuple(d.trainable_params())\n",
    "        self.reducer_flag = False\n",
    "        self.grad_reducer = None\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            if auto_parallel_context().get_device_num_is_set():\n",
    "                degree = context.get_auto_parallel_context(\"device_num\")\n",
    "            else:\n",
    "                degree = get_group_size()\n",
    "            self.grad_reducer = nn.DistributedGradReducer(optimizer.parameters, mean, degree)\n",
    "\n",
    "    def construct(self, img_a, img_b, fake_a, fake_b):\n",
    "        weights = self.weights\n",
    "        ld = self.d(img_a, img_b, fake_a, fake_b)\n",
    "        sens_d = ops.Fill()(ops.DType()(ld), ops.Shape()(ld), self.sens)\n",
    "        grads_d = self.grad(self.d, weights)(img_a, img_b, fake_a, fake_b, sens_d)\n",
    "        if self.reducer_flag:\n",
    "            grads_d = self.grad_reducer(grads_d)\n",
    "        return ops.depend(ld, self.optimizer(grads_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行模型的训练，实例化损失函数，优化器。本案例训练的是resnet生成器网络，patchGAN判别器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "\n",
    "from src.utils.tools import get_lr, ImagePool, load_ckpt\n",
    "from src.utils.cyclegan_losses import DiscriminatorLoss, GeneratorLoss\n",
    "from src.models.cycle_gan import get_generator, get_discriminator, Generator, TrainOneStepG, TrainOneStepD\n",
    "\n",
    "\n",
    "# 设置种子数为1。\n",
    "ms.set_seed(1)\n",
    "\n",
    "# 初始化权重，构建正向反向生成器网络。\n",
    "g_a = get_generator(args)\n",
    "g_b = get_generator(args)\n",
    "\n",
    "# 初始化权重，构建正向反向判别器网络。\n",
    "d_a = get_discriminator(args)\n",
    "d_b = get_discriminator(args)\n",
    "\n",
    "# 是否加载预权重，本案例默认为不需要。\n",
    "if args.load_ckpt:\n",
    "    load_ckpt(args, g_a, g_b, d_a, d_b)\n",
    "\n",
    "# 更新判别器使用生成图像的历史而不是最新生成器生成的图像。文章保留了一个图像缓冲区，用来存储之前创建的50个图像。\n",
    "imgae_pool_a = ImagePool(args.pool_size)\n",
    "imgae_pool_b = ImagePool(args.pool_size)\n",
    "\n",
    "# 实例化生成器。\n",
    "generator = Generator(g_a, g_b, args.lambda_idt > 0)\n",
    "\n",
    "# 创建生成器，判别器损失函数。\n",
    "loss_d = DiscriminatorLoss(args, d_a, d_b)\n",
    "loss_g = GeneratorLoss(args, generator, d_a, d_b)\n",
    "\n",
    "# 构建生成器，判别器优化器。\n",
    "optimizer_g = nn.Adam(generator.trainable_params(), get_lr(args), beta1=args.beta1)\n",
    "optimizer_d = nn.Adam(loss_d.trainable_params(), get_lr(args), beta1=args.beta1)\n",
    "\n",
    "# 构建生成器判别器训练过程。\n",
    "net_g = TrainOneStepG(loss_g, generator, optimizer_g)\n",
    "net_d = TrainOneStepD(loss_d, optimizer_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行训练时的报告生成与保存,以及对生成的图片以及权重文件进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Args:\n",
      "--> phase: train\n",
      "--> platform: GPU\n",
      "--> device_id: 0\n",
      "--> device_num: 1\n",
      "--> is_save_on_master: 1\n",
      "--> rank: 0\n",
      "--> group_size: 1\n",
      "--> init_type: normal\n",
      "--> init_gain: 0.02\n",
      "--> image_size: 256\n",
      "--> batch_size: 1\n",
      "--> pool_size: 50\n",
      "--> beta1: 0.5\n",
      "--> lr: 0.0002\n",
      "--> lr_policy: linear\n",
      "--> max_epoch: 1\n",
      "--> n_epochs: 1\n",
      "--> in_planes: 3\n",
      "--> ngf: 64\n",
      "--> gl_num: 9\n",
      "--> ndf: 64\n",
      "--> dl_num: 3\n",
      "--> slope: 0.2\n",
      "--> norm_mode: instance\n",
      "--> lambda_a: 10.0\n",
      "--> lambda_b: 10.0\n",
      "--> lambda_idt: 0.5\n",
      "--> gan_mode: lsgan\n",
      "--> pad_mode: CONSTANT\n",
      "--> data: horse2zebra.zip\n",
      "--> dataroot: ./data/horse2zebra\n",
      "--> data_dir: testA\n",
      "--> outputs_dir: ./outputs\n",
      "--> load_ckpt: False\n",
      "--> g_a_ckpt: ./outputs/ckpt/g_a_200.ckpt\n",
      "--> g_b_ckpt: ./outputs/ckpt/g_b_200.ckpt\n",
      "--> d_a_ckpt: ./outputs/ckpt/d_a_200.ckpt\n",
      "--> d_b_ckpt: ./outputs/ckpt/d_b_200.ckpt\n",
      "--> save_checkpoint_epochs: 10\n",
      "--> print_iter: 100\n",
      "--> need_profiler: False\n",
      "--> save_graphs: False\n",
      "--> save_imgs: True\n",
      "--> use_random: True\n",
      "--> need_dropout: False\n",
      "--> max_dataset_size: inf\n",
      "--> n_epochs_decay: 0\n",
      "--> dataset_size: 1334\n",
      "\n",
      "==========start training===============\n",
      "Epoch[1] [100/1334] step cost: 285.18 ms, G_loss: 10.50, D_loss:0.63, loss_G_A: 0.43, loss_G_B: 0.41, loss_C_A: 1.49,loss_C_B: 5.01, loss_idt_A: 0.73, loss_idt_B：2.44\n",
      "Epoch[1] [200/1334] step cost: 131.48 ms, G_loss: 12.78, D_loss:0.60, loss_G_A: 0.40, loss_G_B: 0.16, loss_C_A: 2.30,loss_C_B: 5.99, loss_idt_A: 0.95, loss_idt_B：2.99\n",
      "Epoch[1] [300/1334] step cost: 132.02 ms, G_loss: 10.32, D_loss:0.56, loss_G_A: 0.37, loss_G_B: 0.29, loss_C_A: 3.50,loss_C_B: 2.96, loss_idt_A: 1.70, loss_idt_B：1.50\n",
      "Epoch[1] [400/1334] step cost: 132.03 ms, G_loss: 7.65, D_loss:0.54, loss_G_A: 0.32, loss_G_B: 0.45, loss_C_A: 2.37,loss_C_B: 2.29, loss_idt_A: 1.24, loss_idt_B：0.99\n",
      "Epoch[1] [500/1334] step cost: 132.43 ms, G_loss: 5.55, D_loss:0.58, loss_G_A: 0.32, loss_G_B: 0.54, loss_C_A: 1.67,loss_C_B: 1.60, loss_idt_A: 0.72, loss_idt_B：0.71\n",
      "Epoch[1] [600/1334] step cost: 132.65 ms, G_loss: 8.59, D_loss:0.29, loss_G_A: 0.45, loss_G_B: 0.19, loss_C_A: 2.82,loss_C_B: 2.79, loss_idt_A: 1.28, loss_idt_B：1.07\n",
      "Epoch[1] [700/1334] step cost: 132.74 ms, G_loss: 8.46, D_loss:0.42, loss_G_A: 0.28, loss_G_B: 0.30, loss_C_A: 2.10,loss_C_B: 3.19, loss_idt_A: 0.92, loss_idt_B：1.67\n",
      "Epoch[1] [800/1334] step cost: 132.79 ms, G_loss: 9.43, D_loss:0.49, loss_G_A: 0.36, loss_G_B: 0.52, loss_C_A: 3.65,loss_C_B: 2.10, loss_idt_A: 1.77, loss_idt_B：1.02\n",
      "Epoch[1] [900/1334] step cost: 133.09 ms, G_loss: 5.85, D_loss:0.52, loss_G_A: 0.58, loss_G_B: 0.21, loss_C_A: 1.92,loss_C_B: 1.67, loss_idt_A: 0.83, loss_idt_B：0.64\n",
      "Epoch[1] [1000/1334] step cost: 133.14 ms, G_loss: 7.94, D_loss:0.73, loss_G_A: 0.80, loss_G_B: 0.21, loss_C_A: 2.45,loss_C_B: 2.51, loss_idt_A: 0.94, loss_idt_B：1.03\n",
      "Epoch[1] [1100/1334] step cost: 133.33 ms, G_loss: 6.19, D_loss:0.73, loss_G_A: 0.79, loss_G_B: 0.29, loss_C_A: 2.31,loss_C_B: 1.28, loss_idt_A: 1.05, loss_idt_B：0.47\n",
      "Epoch[1] [1200/1334] step cost: 133.24 ms, G_loss: 9.44, D_loss:0.65, loss_G_A: 0.70, loss_G_B: 0.17, loss_C_A: 2.94,loss_C_B: 2.67, loss_idt_A: 1.64, loss_idt_B：1.32\n",
      "Epoch[1] [1300/1334] step cost: 133.73 ms, G_loss: 8.29, D_loss:0.37, loss_G_A: 0.38, loss_G_B: 0.45, loss_C_A: 2.60,loss_C_B: 2.50, loss_idt_A: 1.46, loss_idt_B：0.90\n",
      "Epoch [1] total cost: 192363.72 ms, per step: 144.20 ms, G_loss: 0.60, D_loss: 8.10\n",
      "==========end training===============\n"
     ]
    }
   ],
   "source": [
    "from src.utils.reporter import Reporter\n",
    "\n",
    "\n",
    "#对数据集进行创建迭代来进行训练，以及保存生成的图片以及权重文件。\n",
    "data_loader = ds.create_dict_iterator()\n",
    "reporter = Reporter(args)\n",
    "reporter.info('==========start training===============')\n",
    "for _ in range(args.max_epoch):\n",
    "    reporter.epoch_start()\n",
    "    for data in data_loader:\n",
    "        img_a = data[\"image_A\"]\n",
    "        img_b = data[\"image_B\"]\n",
    "        res_g = net_g(img_a, img_b)\n",
    "        fake_a = res_g[0]\n",
    "        fake_b = res_g[1]\n",
    "        res_d = net_d(img_a, img_b, imgae_pool_a.query(fake_a), imgae_pool_b.query(fake_b))\n",
    "        reporter.step_end(res_g, res_d)\n",
    "        reporter.visualizer(img_a, img_b, fake_a, fake_b)\n",
    "    reporter.epoch_end(net_g)\n",
    "    if args.need_profiler:\n",
    "        profiler.analyse()\n",
    "        break\n",
    "reporter.info('==========end training===============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便展示，案例中将epoch设置为1，经测试，在实际训练中，将epoch设置为200是一个不错的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理\n",
    "\n",
    "下面进行所训练的模型在测试数据集上的推理以及效果评估，用于推理的图片为数据集testA，testB，训练好的权重可以在此处[cyclegan_ckpt](https://download.mindspore.cn/vision/cyclegan/)进行下载，一种数据集风格迁移有四个ckpt，下载完毕后放到src/output/ckpt文件里面，或者放在其他位置将args.py文件中的对应路径进行修改对应即可，本案例对应下载horse2zebra的ckpt文件,其文件夹名字为horse。\n",
    "其中ckpt目录如下所示：\n",
    "\n",
    "```text\n",
    "./outputs/ckpt\n",
    "├── g_a_200.ckpt\n",
    "├── g_b_200.ckpt\n",
    "├── d_a_200.ckpt\n",
    "└── d_b_200.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:49.604.739 [mindspore/train/serialization.py:674] For 'load_param_into_net', remove parameter generator.G_A.conv_in.features.0.weight's prefix name: generator.G_A., continue to load it to net parameter conv_in.features.0.weight.\n",
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:49.616.475 [mindspore/train/serialization.py:674] For 'load_param_into_net', remove parameter generator.G_B.conv_in.features.0.weight's prefix name: generator.G_B., continue to load it to net parameter conv_in.features.0.weight.\n",
      "Args:\n",
      "--> phase: predict\n",
      "--> platform: GPU\n",
      "--> device_id: 0\n",
      "--> device_num: 1\n",
      "--> is_save_on_master: 1\n",
      "--> rank: 0\n",
      "--> group_size: 1\n",
      "--> init_type: normal\n",
      "--> init_gain: 0.02\n",
      "--> image_size: 256\n",
      "--> batch_size: 1\n",
      "--> pool_size: 50\n",
      "--> beta1: 0.5\n",
      "--> lr: 0.0002\n",
      "--> lr_policy: linear\n",
      "--> max_epoch: 1\n",
      "--> n_epochs: 1\n",
      "--> in_planes: 3\n",
      "--> ngf: 64\n",
      "--> gl_num: 9\n",
      "--> ndf: 64\n",
      "--> dl_num: 3\n",
      "--> slope: 0.2\n",
      "--> norm_mode: instance\n",
      "--> lambda_a: 10.0\n",
      "--> lambda_b: 10.0\n",
      "--> lambda_idt: 0.5\n",
      "--> gan_mode: lsgan\n",
      "--> pad_mode: CONSTANT\n",
      "--> data: horse2zebra.zip\n",
      "--> dataroot: ./data/horse2zebra\n",
      "--> data_dir: testA\n",
      "--> outputs_dir: ./outputs\n",
      "--> load_ckpt: False\n",
      "--> g_a_ckpt: ./outputs/ckpt/g_a_200.ckpt\n",
      "--> g_b_ckpt: ./outputs/ckpt/g_b_200.ckpt\n",
      "--> d_a_ckpt: ./outputs/ckpt/d_a_200.ckpt\n",
      "--> d_b_ckpt: ./outputs/ckpt/d_b_200.ckpt\n",
      "--> save_checkpoint_epochs: 10\n",
      "--> print_iter: 100\n",
      "--> need_profiler: False\n",
      "--> save_graphs: False\n",
      "--> save_imgs: True\n",
      "--> use_random: True\n",
      "--> need_dropout: False\n",
      "--> max_dataset_size: inf\n",
      "--> n_epochs_decay: 0\n",
      "--> dataset_size: 120\n",
      "\n",
      "==========start predict A to B===============\n",
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:49.660.454 [mindspore/dataset/engine/datasets_user_defined.py:756] GeneratorDataset's num_parallel_workers: 8 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 5 or smaller.\n",
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:50.586.93 [mindspore/dataset/engine/datasets_user_defined.py:756] GeneratorDataset's num_parallel_workers: 8 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 5 or smaller.\n",
      "save fake_b at ./outputs/predict/fake_b/n02381460_1260.jpg\n",
      "total 120 imgs cost 3881.72 ms, per img cost 32.35\n",
      "==========end predict A to B===============\n",
      "\n",
      "==========start predict B to A===============\n",
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:53.549.192 [mindspore/dataset/engine/datasets_user_defined.py:756] GeneratorDataset's num_parallel_workers: 8 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 5 or smaller.\n",
      "[WARNING] ME(107468:140650712728384,MainProcess):2022-07-07-17:56:53.966.893 [mindspore/dataset/engine/datasets_user_defined.py:756] GeneratorDataset's num_parallel_workers: 8 is too large which may cause a lot of memory occupation (>85%) or out of memory(OOM) during multiprocessing. Therefore, it is recommended to reduce num_parallel_workers to 4 or smaller.\n",
      "save fake_a at ./outputs/predict/fake_a/n02391049_600.jpg\n",
      "total 140 imgs cost 4344.96 ms, per img cost 31.04\n",
      "==========end predict B to A===============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mindspore import Tensor\n",
    "\n",
    "from src.models.cycle_gan import get_generator\n",
    "from src.utils.reporter import Reporter\n",
    "from src.utils.tools import save_image, load_ckpt\n",
    "from src.process_datasets.dataset import create_dataset\n",
    "\n",
    "\n",
    "# 这几步为模型构建以及加载训练好的权重。\n",
    "args.phase = \"predict\"\n",
    "g_a.set_train(False)\n",
    "g_b.set_train(False)\n",
    "load_ckpt(args, g_a, g_b)\n",
    "\n",
    "# 此处为创建推理图片输出位置。\n",
    "imgs_out = os.path.join(args.outputs_dir, \"predict\")\n",
    "if not os.path.exists(imgs_out):\n",
    "    os.makedirs(imgs_out)\n",
    "if not os.path.exists(os.path.join(imgs_out, \"fake_a\")):\n",
    "    os.makedirs(os.path.join(imgs_out, \"fake_a\"))\n",
    "if not os.path.exists(os.path.join(imgs_out, \"fake_b\")):\n",
    "    os.makedirs(os.path.join(imgs_out, \"fake_b\"))\n",
    "\n",
    "# 此处对正向以及反向推理创建数据集并进行推理以及图片保存。\n",
    "args.data_dir = 'testA'\n",
    "ds = create_dataset(args)\n",
    "reporter.start_predict(\"A to B\")\n",
    "for data in ds.create_dict_iterator(output_numpy=True):\n",
    "    img_a = Tensor(data[\"image\"])\n",
    "    path_a = str(data[\"image_name\"][0], encoding=\"utf-8\")\n",
    "    path_b = path_a[0:-4] + \"_fake_b.jpg\"\n",
    "    fake_b = g_a(img_a)\n",
    "    save_image(fake_b, os.path.join(imgs_out, \"fake_b\", path_b))\n",
    "    save_image(img_a, os.path.join(imgs_out, \"fake_b\", path_a))\n",
    "reporter.info('save fake_b at %s', os.path.join(imgs_out, \"fake_b\", path_a))\n",
    "reporter.end_predict()\n",
    "args.data_dir = 'testB'\n",
    "ds = create_dataset(args)\n",
    "reporter.dataset_size = args.dataset_size\n",
    "reporter.start_predict(\"B to A\")\n",
    "for data in ds.create_dict_iterator(output_numpy=True):\n",
    "    img_b = Tensor(data[\"image\"])\n",
    "    path_b = str(data[\"image_name\"][0], encoding=\"utf-8\")\n",
    "    path_a = path_b[0:-4] + \"_fake_a.jpg\"\n",
    "    fake_a = g_b(img_b)\n",
    "    save_image(fake_a, os.path.join(imgs_out, \"fake_a\", path_a))\n",
    "    save_image(img_b, os.path.join(imgs_out, \"fake_b\", path_b))\n",
    "reporter.info('save fake_a at %s', os.path.join(imgs_out, \"fake_a\", path_b))\n",
    "reporter.end_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的数据结果可以到predict文件夹进行查看，这里展示部分推理的结果，原数据集是A域为普通马匹，B域为斑马。第一幅图为原始A域的图片，第二幅图片为由第一幅图生成的一幅B域的fake斑马图片，第三幅图片为原始B域的图片，第四幅为由第三幅图生成的A域的fake普通马图。\n",
    "\n",
    "![horse](./images/horse.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是其他一些风格迁移的效果展示:\n",
    "\n",
    "![style](./images/style.jpg)\n",
    "\n",
    "![change](./images/change.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本案例对cyclegan的论文中提出的模型进行了详细的解释，向读者完整地展现了该算法的流程。如需查看详细代码，可参考[course仓库](https://toscode.gitee.com/mindspore/course/tree/master/application_example/cyclegan)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用\n",
    "\n",
    "[1] Jun-Yan Zhu，Taesung Park，Phillip Isola，Alexei A. Efros Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks[J].arXiv preprint arXiv:1703.10593.pdf\n",
    "[2] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution.In ECCV, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
